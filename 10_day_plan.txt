10-Day Plan (SMS Spam → Security “Threat Detection + Triage” System)
The project (simple, end-to-end, interview-ready)
“Message Threat Detection & Triage Service”

Classic ML: TF‑IDF + Logistic Regression spam/phishing classifier
API (FastAPI):
POST /score → risk score + label + model_version
POST /tag → lightweight tags (rules/regex) like link_present, urgency, money, account_action
POST /feedback → store analyst override labels (feedback loop)
Pipelines: preprocess → train → eval → package artifact
AWS deployment: Docker → ECR → ECS Fargate (plus SageMaker thin slice later)
Monitoring: CloudWatch logs/metrics + drift checks on score distribution
LangSmith day: add an LLM “triage summary” chain and trace/eval it

Day 0 (setup day): dataset + repo + AWS safety rails
A) Dataset (SMS Spam Collection)
Get it from UCI:
Search: “UCI SMS Spam Collection dataset”
Download the zip that contains SMSSpamCollection
Put it in your repo under something like:
data/raw/SMSSpamCollection
Create a tiny processing script idea:
reads the file (tab-separated: label \t text)
outputs data/processed/sms_spam.csv
Important: don’t commit raw data if you don’t want to—commit a small sample + a script that downloads it.
B) Local dev environment (fast)
Create a new repo (or a new folder) for this project.
Set up:
Python venv
requirements.txt (FastAPI, uvicorn, scikit-learn, pandas, joblib, pytest, etc.)
A Makefile or simple scripts/ commands (optional but nice)
C) AWS account + cost controls (must-do)
Create AWS account
Set AWS Budget:
budget: $100
alerts: $50 / $80 / $100
Pick one region (e.g. us-east-1) and stick to it.
Tagging convention:
project=sms-threat-demo
owner=muriel
expiry=YYYY-MM-DD
Decide your “shutdown habit”:
ECS service scaled to 0 overnight (unless you need it live)
SageMaker endpoints always deleted the same day
D) AWS technical prerequisites (keep minimal)
Install/verify locally:
AWS CLI
Docker
In AWS:
Create an ECR repository (name like sms-threat-demo)
Create an IAM user/role setup you can use for pushing to ECR (don’t overcomplicate; just enough permissions)
Definition of done for Day 0

Dataset downloaded + readable
AWS budget alerts set
ECR repo exists
You can run docker build locally successfully (even before deployment)

Day 1: Baseline model 
TF-IDF + Logistic Regression pipeline:
- Train/val/test splits with proper data leakage prevention
- Model comparison framework (Logistic Regression vs Random Forest)
- F1: 0.942 (best performer)

Deliverables :
- training/train.py, training/compare_models.py
- artifacts/model.joblib (+ S3 storage)
- reports/metrics.json, reports/model_comparison.json

Day 2: Feature engineering + "tagging" layer 
19 heuristic features for fast, interpretable spam detection:
- Link detection, financial content, urgency indicators, account security, meta features
- Enhanced Random Forest model combining TF-IDF + heuristic features (F1: 0.928)
- Clean EnhancedSpamPipeline architecture with encapsulated feature extraction

Deliverables :
- features/tagging.py (heuristic feature extraction module)
- training/train_enhanced.py
- artifacts/enhanced_model.joblib (+ S3 storage)
- reports/enhanced_metrics.json, reports/feature_engineering_report.md
- Updated README.md with comprehensive documentation

Day 3: FastAPI service + tests (production shape)
Endpoints:

POST /score:
returns risk_score, is_threat, model_version
POST /tag:
returns tags + reasons
POST /feedback:
stores {message_id, text, true_label} to a local sqlite or simple file (keep it easy)
Add:

structured JSON logging
pytest smoke tests
Deliverables

app/main.py
tests/test_api.py
local run: uvicorn app.main:app
Day 4: Dockerize + ECS Fargate deploy + Secrets Management (core new-job readiness)
Dockerize FastAPI
Push image to ECR
ECS task definition + service with AWS Secrets Manager integration
Secrets management:

AWS Secrets Manager for sensitive configuration
Environment variables for non-sensitive config
Model registry credentials (S3 access)
Database connection strings (future-proofing)
API keys for external services
Keep networking simple; add ALB only if necessary
Deliverables

Public endpoint (or at least reachable endpoint)
deploy/deploy_notes.md with exact steps and rollback notes
deploy/secrets_management.md (secrets strategy and rotation)
Day 5: CI/CD (GitHub Actions)
Pipeline:

run tests
build Docker image
push to ECR on merge to main Optional:
manual deploy trigger
Deliverables

.github/workflows/ci.yml (or ci_cd.yml)
passing green build
Day 6: Feature Store Implementation (production ML patterns)
Build a lightweight feature store for heuristic features:

Core components:
- Feature computation service (batch + real-time)
- Feature storage (Redis/DynamoDB for online, S3 for offline)
- Feature serving API with caching
- Feature versioning and lineage

Implementation:
- Refactor heuristic features to be computed separately
- Add /compute-features endpoint
- Cache features by message hash
- Batch feature computation for training data
- Feature drift monitoring

Deliverables:
features/store.py (feature store client)
features/compute.py (batch feature computation)
feature_store_design.md (architecture decisions)

Day 7: Versioning with MLflow (thin but credible)
Track:
dataset hash/version
vectorizer params
model params + feature store version
metrics
Register a model version
Make your service load model by MODEL_VERSION env var (even if it maps to a local artifact in the container for now—fine for a demo)
Deliverables

mlflow_notes.md
"How to rollback model" section in README
Day 8: Performance Optimization + Scaling (Applied ML Engineering focus)
Performance optimization:

ThreadPoolExecutor for CPU-bound ML operations
Async optimization for concurrent request handling
Batch processing endpoint for efficient multi-message scoring
Memory optimization and resource management
Load testing & scaling:

Load testing with locust/artillery (simulate 2k RPS)
Latency percentile analysis (p50, p95, p99)
ECS auto-scaling configuration and testing
Horizontal scaling with multiple instances
Performance monitoring:

CPU utilization tracking
Memory usage patterns
Request queue depth monitoring
Throughput vs latency trade-offs
Deliverables

performance/load_test.py (load testing scripts)
performance/optimization_report.md (bottleneck analysis)
Updated app/main.py with ThreadPoolExecutor
ECS scaling policies and configuration
Day 9: Production Monitoring + Drift Detection (Applied ML Engineering focus)
Service monitoring:

Real-time latency and throughput dashboards
Error rate tracking and alerting
Resource utilization monitoring (CPU, memory, network)
Request queue depth and processing time
Model monitoring:

Prediction drift detection (PSI on risk_score distribution)
Feature drift monitoring (heuristic feature frequency changes)
Model performance degradation alerts
A/B testing framework for model versions
Feedback loop integration:

Automated retraining triggers based on feedback volume
Data quality monitoring on incoming requests
Performance regression detection
Deliverables

monitoring/dashboards.py (CloudWatch dashboard setup)
monitoring/drift_detection.py (model drift monitoring)
monitoring/alerts.yaml (alerting configuration)
monitoring_strategy.md (comprehensive monitoring approach)

Day 10: Modern AI Stack Evaluation (2026 Production Tools)
Compare and implement multiple modern AI/LLM platforms:

LLMOps Platform Comparison:
- PostHog: Product analytics + LLM observability integration
- Braintrust: Evaluation-first approach with Loop AI agent
- LangSmith: LangChain-native tracing and evaluation
Agent Framework Evaluation:
- CrewAI: Multi-agent collaboration system
- AutoGen: Microsoft's enterprise multi-agent framework
- LangGraph: State-based agent workflows
Implementation:
- Add LLM triage endpoint: POST /triage-summary
- Integrate PostHog for user behavior + AI metrics
- Compare evaluation approaches across platforms
- A/B test different prompt strategies
Deliverables

ai_stack_comparison.md (comprehensive tool evaluation matrix)
posthog_integration.py (analytics + LLM observability)
triage_endpoint.py (LLM-powered message analysis)
evaluation_report.md (platform comparison results)
production_ai_recommendations.md (2026 tooling strategy)
Daily shutdown checklist (do this every evening)
ECS: scale service to 0 if not needed overnight
SageMaker: delete endpoints / stop notebooks
Check billing dashboard + budget alerts
Status
Complete: Updated 10-day plan reflecting actual progress (Day 1 baseline, Day 2 feature engineering). Documentation will be maintained throughout the project rather than as a separate final day.
