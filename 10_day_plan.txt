10-Day Plan (SMS Spam → Security “Threat Detection + Triage” System)
The project (simple, end-to-end, interview-ready)
“Message Threat Detection & Triage Service”

Classic ML: TF‑IDF + Logistic Regression spam/phishing classifier
API (FastAPI):
POST /score → risk score + label + model_version
POST /tag → lightweight tags (rules/regex) like link_present, urgency, money, account_action
POST /feedback → store analyst override labels (feedback loop)
Pipelines: preprocess → train → eval → package artifact
AWS deployment: Docker → ECR → ECS Fargate (plus SageMaker thin slice later)
Monitoring: CloudWatch logs/metrics + drift checks on score distribution
LangSmith day: add an LLM “triage summary” chain and trace/eval it

Day 0 (setup day): dataset + repo + AWS safety rails
A) Dataset (SMS Spam Collection)
Get it from UCI:
Search: “UCI SMS Spam Collection dataset”
Download the zip that contains SMSSpamCollection
Put it in your repo under something like:
data/raw/SMSSpamCollection
Create a tiny processing script idea:
reads the file (tab-separated: label \t text)
outputs data/processed/sms_spam.csv
Important: don’t commit raw data if you don’t want to—commit a small sample + a script that downloads it.
B) Local dev environment (fast)
Create a new repo (or a new folder) for this project.
Set up:
Python venv
requirements.txt (FastAPI, uvicorn, scikit-learn, pandas, joblib, pytest, etc.)
A Makefile or simple scripts/ commands (optional but nice)
C) AWS account + cost controls (must-do)
Create AWS account
Set AWS Budget:
budget: $100
alerts: $50 / $80 / $100
Pick one region (e.g. us-east-1) and stick to it.
Tagging convention:
project=sms-threat-demo
owner=muriel
expiry=YYYY-MM-DD
Decide your “shutdown habit”:
ECS service scaled to 0 overnight (unless you need it live)
SageMaker endpoints always deleted the same day
D) AWS technical prerequisites (keep minimal)
Install/verify locally:
AWS CLI
Docker
In AWS:
Create an ECR repository (name like sms-threat-demo)
Create an IAM user/role setup you can use for pushing to ECR (don’t overcomplicate; just enough permissions)
Definition of done for Day 0

Dataset downloaded + readable
AWS budget alerts set
ECR repo exists
You can run docker build locally successfully (even before deployment)

Day 1: Baseline model 
TF-IDF + Logistic Regression pipeline:
- Train/val/test splits with proper data leakage prevention
- Model comparison framework (Logistic Regression vs Random Forest)
- F1: 0.942 (best performer)

Deliverables :
- training/train.py, training/compare_models.py
- artifacts/model.joblib (+ S3 storage)
- reports/metrics.json, reports/model_comparison.json

Day 2: Feature engineering + "tagging" layer 
19 heuristic features for fast, interpretable spam detection:
- Link detection, financial content, urgency indicators, account security, meta features
- Enhanced Random Forest model combining TF-IDF + heuristic features (F1: 0.928)
- Clean EnhancedSpamPipeline architecture with encapsulated feature extraction

Deliverables :
- features/tagging.py (heuristic feature extraction module)
- training/train_enhanced.py
- artifacts/enhanced_model.joblib (+ S3 storage)
- reports/enhanced_metrics.json, reports/feature_engineering_report.md
- Updated README.md with comprehensive documentation

Day 3: FastAPI service + tests (production shape)
Endpoints:

POST /score:
returns risk_score, is_threat, model_version
POST /tag:
returns tags + reasons
POST /feedback:
stores {message_id, text, true_label} to a local sqlite or simple file (keep it easy)
Add:

structured JSON logging
pytest smoke tests
Deliverables

app/main.py
tests/test_api.py
local run: uvicorn app.main:app
Day 4: Dockerize + ECS Fargate deploy (core new-job readiness)
Dockerize FastAPI
Push image to ECR
ECS task definition + service
Keep networking simple; add ALB only if necessary
Deliverables

Public endpoint (or at least reachable endpoint)
deploy/deploy_notes.md with exact steps and rollback notes
Day 5: CI/CD (GitHub Actions)
Pipeline:

run tests
build Docker image
push to ECR on merge to main Optional:
manual deploy trigger
Deliverables

.github/workflows/ci.yml (or ci_cd.yml)
passing green build
Day 6: Feature Store Implementation (production ML patterns)
Build a lightweight feature store for heuristic features:

Core components:
- Feature computation service (batch + real-time)
- Feature storage (Redis/DynamoDB for online, S3 for offline)
- Feature serving API with caching
- Feature versioning and lineage

Implementation:
- Refactor heuristic features to be computed separately
- Add /compute-features endpoint
- Cache features by message hash
- Batch feature computation for training data
- Feature drift monitoring

Deliverables:
features/store.py (feature store client)
features/compute.py (batch feature computation)
feature_store_design.md (architecture decisions)

Day 7: Versioning with MLflow (thin but credible)
Track:
dataset hash/version
vectorizer params
model params + feature store version
metrics
Register a model version
Make your service load model by MODEL_VERSION env var (even if it maps to a local artifact in the container for now—fine for a demo)
Deliverables

mlflow_notes.md
"How to rollback model" section in README
Day 8: Monitoring + drift + feedback loop (security flavor)
Service monitoring:

latency
error rate
request volume
Model monitoring:

drift of risk_score distribution (PSI)
drift in top TF‑IDF features frequency (optional—skip if time)
Feedback loop:

/feedback writes a record you can later use to retrain (Day 9/10 optional)
Deliverables

monitoring.md describing:
what you monitor, why
how you’d respond to drift
how feedback becomes training data
Day 9: SageMaker thin slice (optional but recommended)
Since this is text ML, SageMaker is less “natural” than for tabular, but still doable:

Deploy the model as a SageMaker endpoint briefly
Test, document, delete
Deliverable

ecs_vs_sagemaker.md with a clear decision framework
(If time is tight, you can swap this day for more monitoring/pipeline polish.)

Day 10: LangSmith day (LLM triage summary + eval)
Add an LLM endpoint:

POST /triage-summary:
input: message text + model score + tags
output: short summary + recommended action (“block”, “send to review”, “allow”) Instrument with LangSmith:
traces
dataset of 10–20 messages
compare two prompt versions
Deliverables

LangSmith traces + a short eval report
langsmith_notes.md (what you learned, how you’d use it in prod)
Daily shutdown checklist (do this every evening)
ECS: scale service to 0 if not needed overnight
SageMaker: delete endpoints / stop notebooks
Check billing dashboard + budget alerts
Status
Complete: Updated 10-day plan reflecting actual progress (Day 1 baseline, Day 2 feature engineering). Documentation will be maintained throughout the project rather than as a separate final day.
